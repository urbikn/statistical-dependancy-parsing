{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very quick and dirty implementation of a neural parser using BERT base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "from src.dataset import ConllDataset\n",
    "\n",
    "class TorchConllDataset(Dataset):\n",
    "    def __init__(self, dataset_file, tokenizer, max_len) -> None:\n",
    "        original_file = 'data/english/train/wsj_train.first-1k.conll06'\n",
    "        self.dataset = ConllDataset(dataset_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      dataset_sentence = self.dataset[index]\n",
    "\n",
    "      sentence = dataset_sentence.sentence['form'].to_list()\n",
    "      arcs = dataset_sentence.get_arcs()\n",
    "\n",
    "      encoding = self.tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 512,           # Pad & truncate all sentences.\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "      )\n",
    "\n",
    "      word_ids = torch.asarray([id if id != None else -1 for id in encoding.word_ids()])\n",
    "      # Padd with -1 to have equal size\n",
    "      arcs = arcs + [[-1, -1]] * (len(word_ids) - len(arcs))\n",
    "      arcs = torch.asarray(arcs)\n",
    "\n",
    "      return {\n",
    "        'word_ids': word_ids,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_masks': encoding['attention_mask'].flatten(),\n",
    "        'arcs': arcs,\n",
    "      }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_dataloader(cls, dataset_file, tokenizer, max_len, batch_size, random=False):\n",
    "      dataset = TorchConllDataset(dataset_file, tokenizer, max_len)\n",
    "\n",
    "      return DataLoader(dataset, batch_size=batch_size, shuffle=random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.decoder as decoder\n",
    "\n",
    "class NeuralDependencyParser(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size = 256):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.bert.trainable = False\n",
    "        self.linear_first = nn.Linear(self.bert.config.hidden_size * 2, hidden_size)\n",
    "        self.linear_second = nn.Linear(hidden_size, 1)\n",
    "        self.function = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0) # Softmax column-wise, since every dependent has only one head\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.decoder = decoder.CLE_n()\n",
    "\n",
    "    def tensors_from_words(self, output, word_id):\n",
    "        new_output = []\n",
    "\n",
    "        prev_word_id = -1\n",
    "        for logit, word_id in zip(output, word_id):\n",
    "            # Mean join tensors that belong to the same word\n",
    "            if word_id == prev_word_id and len(new_output):\n",
    "                new_output[-1] = torch.stack([new_output[-1], logit]).mean(dim=0)\n",
    "            else:\n",
    "                new_output.append(logit)\n",
    "            \n",
    "            prev_word_id = word_id\n",
    "\n",
    "        # Gets tensors for input tokens (removes special [SEP])\n",
    "        output = torch.stack(new_output)[:-1]\n",
    "        return output\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, word_ids):\n",
    "        with torch.no_grad():\n",
    "            logits = self.bert(input_ids=input_ids, attention_mask=attention_masks)[0]\n",
    "\n",
    "        batch_scores = []\n",
    "\n",
    "        # Go through the batch and create scores\n",
    "        for logit, attention_mask, word_id in zip(logits, attention_masks, word_ids):\n",
    "            # Only get non-padded values\n",
    "            output = logit[torch.where(attention_mask == 1)]\n",
    "\n",
    "            # Join subword tensors into one to represent one word\n",
    "            word_output = self.tensors_from_words(output, word_id)\n",
    "\n",
    "            arcs = []\n",
    "            for h, head in enumerate(word_output):\n",
    "                head_arcs = []\n",
    "                for d, dep in enumerate(word_output):\n",
    "                    if h != d and d != 0: # Don't want the same concatinated and the first (meaning root is dep)\n",
    "                        head_dep_arc = torch.cat((head, dep))\n",
    "                    else:\n",
    "                        head_dep_arc = torch.zeros((head.shape[0] * 2)).to(device)\n",
    "\n",
    "                    head_arcs.append(head_dep_arc)\n",
    "\n",
    "                arcs.append(torch.stack(head_arcs))\n",
    "\n",
    "\n",
    "            arcs = torch.stack(arcs).to(device)\n",
    "            h = self.function(self.linear_first(arcs))\n",
    "            scores = self.softmax(self.linear_second(h)).flatten(start_dim=1)\n",
    "            batch_scores.append(scores)\n",
    "\n",
    "            del output\n",
    "            del word_output\n",
    "            del arcs\n",
    "\n",
    "\n",
    "        return batch_scores\n",
    "\n",
    "    def loss_fn(self, batch_scores, batch_arcs):\n",
    "        losses = []\n",
    "        for scores, arcs in zip(batch_scores, batch_arcs):\n",
    "            true_scores = torch.zeros(scores.shape).to(device)\n",
    "            for head, dep in arcs:\n",
    "                # Since we padded the arcs, when we get [-1, -1], we stop\n",
    "                if head == -1 and dep == -1:\n",
    "                    break\n",
    "                true_scores[head, dep] = 1\n",
    "\n",
    "            loss = self.loss(scores, true_scores)\n",
    "            losses.append(loss)\n",
    "\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "    def decode(self, batch_scores):\n",
    "        predicted_tree = []\n",
    "        for scores in batch_scores:\n",
    "            parser_scores = torch.clone(scores).detach().cpu().numpy()\n",
    "            parser_scores[:, 0] = -np.Inf\n",
    "            parser_scores[np.diag_indices_from(parser_scores)] = -np.Inf\n",
    "\n",
    "            tree = self.decoder.decode(parser_scores)\n",
    "            predicted_tree.append(tree)\n",
    "\n",
    "        return predicted_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "training_file = 'data/english/train/wsj_train.conll06'\n",
    "dev_file = 'data/english/dev/wsj_dev.conll06.gold'\n",
    "test_file = 'data/english/dev/wsj_dev.conll06.gold'\n",
    "\n",
    "train_dataloader = TorchConllDataset.get_dataloader(training_file, tokenizer, 512, 64, random=True)\n",
    "dev_dataloader = TorchConllDataset.get_dataloader(dev_file, tokenizer, 512, 32)\n",
    "test_dataloader = TorchConllDataset.get_dataloader(dev_file, tokenizer, 512, 8)\n",
    "\n",
    "model = NeuralDependencyParser('bert-base-cased', 512)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = TorchConllDataset.get_dataloader(training_file, tokenizer, 512, 32, random=True)\n",
    "dev_dataloader = TorchConllDataset.get_dataloader(dev_file, tokenizer, 512, 32)\n",
    "test_dataloader = TorchConllDataset.get_dataloader(dev_file, tokenizer, 512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 721/1880 [09:54<16:28,  1.17it/s, score=0, loss=2.38]"
     ]
    }
   ],
   "source": [
    "import src.evaluation as evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "postfix_data = {'score': 0, 'loss': 0}\n",
    "\n",
    "model.train()\n",
    "with tqdm(range(num_training_steps), total=num_training_steps, postfix=postfix_data) as progress_bar:\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_trees = []\n",
    "        true_trees = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            scores= model(\n",
    "                input_ids = batch['input_ids'].to(device),\n",
    "                attention_masks = batch['attention_masks'].to(device),\n",
    "                word_ids = batch['word_ids'].to(device),\n",
    "            )\n",
    "\n",
    "            loss = model.loss_fn(scores, batch['arcs'].to(device))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            postfix_data['loss'] = loss.item()\n",
    "            progress_bar.set_postfix(postfix_data)\n",
    "\n",
    "        model.eval()\n",
    "        with tqdm(dev_dataloader, total=len(dev_dataloader)) as dev_progress_bar:\n",
    "            true_trees = []\n",
    "            predited_trees = []\n",
    "            for batch in dev_progress_bar:\n",
    "                scores= model(\n",
    "                    input_ids = batch['input_ids'].to(device),\n",
    "                    attention_masks = batch['attention_masks'].to(device),\n",
    "                    word_ids = batch['word_ids'].to(device),\n",
    "                )\n",
    "\n",
    "                true_arcs = []\n",
    "                for arcs in batch['arcs']:\n",
    "                    index = -1\n",
    "                    for i, (head, dep) in enumerate(arcs):\n",
    "                        # Since we padded the arcs, when we get [-1, -1], we stop\n",
    "                        if head == -1 and dep == -1:\n",
    "                            index = i\n",
    "                            break\n",
    "\n",
    "                    true_arcs.append(arcs[:i].tolist())\n",
    "                predicted_arcs = model.decode(scores)\n",
    "\n",
    "                true_trees.extend(true_arcs)\n",
    "                pred_trees.extend(predicted_arcs)\n",
    "            \n",
    "            score = evaluation.uas(true_trees, pred_trees)\n",
    "            postfix_data['score'] = score\n",
    "            progress_bar.set_postfix(postfix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:29<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.78717804098515"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with tqdm(dev_dataloader, total=len(dev_dataloader)) as dev_progress_bar:\n",
    "    true_trees = []\n",
    "    predited_trees = []\n",
    "    for batch in dev_progress_bar:\n",
    "        scores= model(\n",
    "            input_ids = batch['input_ids'].to(device),\n",
    "            attention_masks = batch['attention_masks'].to(device),\n",
    "            word_ids = batch['word_ids'].to(device),\n",
    "        )\n",
    "\n",
    "        true_arcs = []\n",
    "        for arcs in batch['arcs']:\n",
    "            index = -1\n",
    "            for i, (head, dep) in enumerate(arcs):\n",
    "                # Since we padded the arcs, when we get [-1, -1], we stop\n",
    "                if head == -1 and dep == -1:\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "            true_arcs.append(arcs[:i].tolist())\n",
    "        predicted_arcs = model.decode(scores)\n",
    "\n",
    "        true_trees.extend(true_arcs)\n",
    "        pred_trees.extend(predicted_arcs)\n",
    "    \n",
    "    score = evaluation.uas(true_trees, pred_trees)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataset = ConllDataset(test_file)\n",
    "with tqdm(test_dataloader, total=len(test_dataloader)) as progress_bar:\n",
    "    true_trees = []\n",
    "    predited_trees = []\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        scores= model(\n",
    "            input_ids = batch['input_ids'].to(device),\n",
    "            attention_masks = batch['attention_masks'].to(device),\n",
    "            word_ids = batch['word_ids'].to(device),\n",
    "        )\n",
    "\n",
    "        predicted_arcs = model.decode(scores)\n",
    "        predicted_tree = sorted(predicted_arcs[0], key=lambda x: x[1])\n",
    "        dataset.set_arcs(sentence_index=i, arcs=predicted_tree)\n",
    "        \n",
    "    \n",
    "dataset.write(filepath='./results/evaluation-test-en-neural.conll06')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b5cba8825993ec79e4a011c10322d669802b2af8cf84efb3d3ee7502f05587e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
